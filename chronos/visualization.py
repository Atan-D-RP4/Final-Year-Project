"""
Advanced Visualization Module for Chronos Forecasting
Provides comprehensive plotting capabilities for time series analysis and forecasting results
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Any, Optional, Tuple, Union
import warnings

# Try to import plotly for interactive plots
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.offline as pyo
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("Warning: plotly not available. Interactive plots will not be available.")

warnings.filterwarnings("ignore")

# Set style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")


class ForecastVisualizer:
    """
    Comprehensive visualization for forecasting results
    """
    
    def __init__(self, figsize: Tuple[int, int] = (12, 8), style: str = 'seaborn'):
        """
        Initialize visualizer
        
        Args:
            figsize: Default figure size
            style: Matplotlib style
        """
        self.figsize = figsize
        self.style = style
        plt.style.use(style)
    
    def plot_time_series(self, 
                        data: Union[List[float], Dict[str, List[float]]], 
                        title: str = "Time Series Data",
                        labels: Optional[List[str]] = None,
                        save_path: Optional[str] = None,
                        show_trend: bool = False,
                        show_seasonal: bool = False) -> plt.Figure:
        """
        Plot time series data
        
        Args:
            data: Time series data (single series or multiple series)
            title: Plot title
            labels: Series labels
            save_path: Path to save figure
            show_trend: Whether to show trend line
            show_seasonal: Whether to show seasonal decomposition
            
        Returns:
            Matplotlib figure
        """
        fig, ax = plt.subplots(figsize=self.figsize)
        
        if isinstance(data, dict):
            # Multiple series
            for i, (name, series) in enumerate(data.items()):
                ax.plot(series, label=name, alpha=0.8, linewidth=2)
        else:
            # Single series
            ax.plot(data, label=labels[0] if labels else 'Data', alpha=0.8, linewidth=2)
            
            if show_trend:
                # Add trend line
                x = np.arange(len(data))
                z = np.polyfit(x, data, 1)
                p = np.poly1d(z)
                ax.plot(x, p(x), '--', alpha=0.7, label='Trend')
        
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlabel('Time', fontsize=12)
        ax.set_ylabel('Value', fontsize=12)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig
    
    def plot_forecast_results(self, 
                            actual: List[float],
                            predicted: List[float],
                            train_data: Optional[List[float]] = None,
                            confidence_intervals: Optional[Dict[str, List[float]]] = None,
                            title: str = "Forecast Results",
                            save_path: Optional[str] = None) -> plt.Figure:
        """
        Plot forecasting results with actual vs predicted
        
        Args:
            actual: Actual values
            predicted: Predicted values
            train_data: Training data (optional)
            confidence_intervals: Dict with 'lower' and 'upper' bounds
            title: Plot title
            save_path: Path to save figure
            
        Returns:
            Matplotlib figure
        """
        fig, ax = plt.subplots(figsize=self.figsize)
        
        # Plot training data if provided
        if train_data:
            train_x = range(len(train_data))
            ax.plot(train_x, train_data, 'b-', alpha=0.6, label='Training Data')
            forecast_start = len(train_data)
        else:
            forecast_start = 0
        
        # Plot actual and predicted
        forecast_x = range(forecast_start, forecast_start + len(actual))
        ax.plot(forecast_x, actual, 'g-', linewidth=2, label='Actual', marker='o', markersize=4)
        ax.plot(forecast_x, predicted, 'r--', linewidth=2, label='Predicted', marker='s', markersize=4)
        \n        # Plot confidence intervals if provided\n        if confidence_intervals:\n            ax.fill_between(forecast_x, \n                           confidence_intervals['lower'], \n                           confidence_intervals['upper'],\n                           alpha=0.3, color='red', label='Confidence Interval')\n        \n        # Add vertical line to separate train/test\n        if train_data:\n            ax.axvline(x=forecast_start, color='black', linestyle=':', alpha=0.7, label='Forecast Start')\n        \n        ax.set_title(title, fontsize=16, fontweight='bold')\n        ax.set_xlabel('Time', fontsize=12)\n        ax.set_ylabel('Value', fontsize=12)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return fig\n    \n    def plot_residual_analysis(self, \n                              actual: List[float],\n                              predicted: List[float],\n                              title: str = \"Residual Analysis\",\n                              save_path: Optional[str] = None) -> plt.Figure:\n        \"\"\"\n        Plot comprehensive residual analysis\n        \n        Args:\n            actual: Actual values\n            predicted: Predicted values\n            title: Plot title\n            save_path: Path to save figure\n            \n        Returns:\n            Matplotlib figure\n        \"\"\"\n        residuals = np.array(actual) - np.array(predicted)\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        fig.suptitle(title, fontsize=16, fontweight='bold')\n        \n        # 1. Residuals over time\n        axes[0, 0].plot(residuals, 'b-', alpha=0.7)\n        axes[0, 0].axhline(y=0, color='r', linestyle='--')\n        axes[0, 0].set_title('Residuals Over Time')\n        axes[0, 0].set_xlabel('Time')\n        axes[0, 0].set_ylabel('Residual')\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # 2. Residual histogram\n        axes[0, 1].hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n        axes[0, 1].axvline(x=0, color='r', linestyle='--')\n        axes[0, 1].set_title('Residual Distribution')\n        axes[0, 1].set_xlabel('Residual')\n        axes[0, 1].set_ylabel('Frequency')\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # 3. Q-Q plot\n        from scipy import stats\n        stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n        axes[1, 0].set_title('Q-Q Plot (Normal Distribution)')\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # 4. Residuals vs fitted values\n        axes[1, 1].scatter(predicted, residuals, alpha=0.6)\n        axes[1, 1].axhline(y=0, color='r', linestyle='--')\n        axes[1, 1].set_title('Residuals vs Fitted Values')\n        axes[1, 1].set_xlabel('Fitted Values')\n        axes[1, 1].set_ylabel('Residuals')\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return fig\n    \n    def plot_model_comparison(self, \n                             results: Dict[str, Dict[str, Any]],\n                             metrics: List[str] = ['mse', 'mae', 'mape', 'mase'],\n                             title: str = \"Model Comparison\",\n                             save_path: Optional[str] = None) -> plt.Figure:\n        \"\"\"\n        Plot comparison of multiple models\n        \n        Args:\n            results: Dictionary with model results\n            metrics: List of metrics to compare\n            title: Plot title\n            save_path: Path to save figure\n            \n        Returns:\n            Matplotlib figure\n        \"\"\"\n        n_metrics = len(metrics)\n        fig, axes = plt.subplots(1, n_metrics, figsize=(4 * n_metrics, 6))\n        if n_metrics == 1:\n            axes = [axes]\n        \n        fig.suptitle(title, fontsize=16, fontweight='bold')\n        \n        model_names = list(results.keys())\n        \n        for i, metric in enumerate(metrics):\n            values = []\n            labels = []\n            \n            for model_name in model_names:\n                if metric in results[model_name]:\n                    value = results[model_name][metric]\n                    if not np.isnan(value) and not np.isinf(value):\n                        values.append(value)\n                        labels.append(model_name)\n            \n            if values:\n                bars = axes[i].bar(range(len(values)), values)\n                axes[i].set_title(f'{metric.upper()}')\n                axes[i].set_xticks(range(len(labels)))\n                axes[i].set_xticklabels(labels, rotation=45, ha='right')\n                axes[i].grid(True, alpha=0.3)\n                \n                # Color bars based on performance (lower is better for most metrics)\n                if metric.lower() in ['mse', 'mae', 'mape', 'mase', 'rmse']:\n                    # Lower is better - color best performance in green\n                    best_idx = np.argmin(values)\n                    for j, bar in enumerate(bars):\n                        if j == best_idx:\n                            bar.set_color('green')\n                        else:\n                            bar.set_color('lightblue')\n                else:\n                    # Higher is better - color best performance in green\n                    best_idx = np.argmax(values)\n                    for j, bar in enumerate(bars):\n                        if j == best_idx:\n                            bar.set_color('green')\n                        else:\n                            bar.set_color('lightblue')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return fig\n    \n    def plot_feature_importance(self, \n                               importance_dict: Dict[str, float],\n                               title: str = \"Feature Importance\",\n                               top_n: int = 15,\n                               save_path: Optional[str] = None) -> plt.Figure:\n        \"\"\"\n        Plot feature importance\n        \n        Args:\n            importance_dict: Dictionary mapping feature names to importance scores\n            title: Plot title\n            top_n: Number of top features to show\n            save_path: Path to save figure\n            \n        Returns:\n            Matplotlib figure\n        \"\"\"\n        if not importance_dict:\n            print(\"No feature importance data to plot\")\n            return None\n        \n        # Sort features by importance\n        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n        \n        features, scores = zip(*sorted_features)\n        \n        fig, ax = plt.subplots(figsize=(10, max(6, len(features) * 0.4)))\n        \n        y_pos = np.arange(len(features))\n        bars = ax.barh(y_pos, scores)\n        \n        # Color bars in gradient\n        colors = plt.cm.viridis(np.linspace(0, 1, len(features)))\n        for bar, color in zip(bars, colors):\n            bar.set_color(color)\n        \n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(features)\n        ax.invert_yaxis()  # Top feature at top\n        ax.set_xlabel('Importance Score')\n        ax.set_title(title, fontsize=16, fontweight='bold')\n        ax.grid(True, alpha=0.3, axis='x')\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return fig\n    \n    def plot_cross_validation_results(self, \n                                     cv_results: Dict[str, Any],\n                                     title: str = \"Cross-Validation Results\",\n                                     save_path: Optional[str] = None) -> plt.Figure:\n        \"\"\"\n        Plot cross-validation results\n        \n        Args:\n            cv_results: Cross-validation results dictionary\n            title: Plot title\n            save_path: Path to save figure\n            \n        Returns:\n            Matplotlib figure\n        \"\"\"\n        if 'split_results' not in cv_results:\n            print(\"No split results found in CV results\")\n            return None\n        \n        split_results = cv_results['split_results']\n        \n        # Extract metrics across splits\n        metrics_data = {}\n        for split in split_results:\n            for metric, value in split['metrics'].items():\n                if metric not in metrics_data:\n                    metrics_data[metric] = []\n                metrics_data[metric].append(value)\n        \n        # Create subplots\n        n_metrics = len(metrics_data)\n        n_cols = min(3, n_metrics)\n        n_rows = (n_metrics + n_cols - 1) // n_cols\n        \n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n        if n_metrics == 1:\n            axes = [axes]\n        elif n_rows == 1:\n            axes = axes.reshape(1, -1)\n        \n        fig.suptitle(title, fontsize=16, fontweight='bold')\n        \n        for i, (metric, values) in enumerate(metrics_data.items()):\n            row = i // n_cols\n            col = i % n_cols\n            ax = axes[row, col] if n_rows > 1 else axes[col]\n            \n            # Filter out invalid values\n            valid_values = [v for v in values if not np.isnan(v) and not np.isinf(v)]\n            \n            if valid_values:\n                splits = range(1, len(valid_values) + 1)\n                ax.plot(splits, valid_values, 'o-', linewidth=2, markersize=6)\n                ax.set_title(f'{metric.upper()}')\n                ax.set_xlabel('Split')\n                ax.set_ylabel(metric.upper())\n                ax.grid(True, alpha=0.3)\n                \n                # Add mean line\n                mean_value = np.mean(valid_values)\n                ax.axhline(y=mean_value, color='red', linestyle='--', alpha=0.7, \n                          label=f'Mean: {mean_value:.4f}')\n                ax.legend()\n        \n        # Hide empty subplots\n        for i in range(n_metrics, n_rows * n_cols):\n            row = i // n_cols\n            col = i % n_cols\n            if n_rows > 1:\n                axes[row, col].set_visible(False)\n            elif n_cols > 1:\n                axes[col].set_visible(False)\n        \n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        \n        return fig\n\n\nclass InteractivePlotter:\n    \"\"\"\n    Interactive plotting using Plotly (if available)\n    \"\"\"\n    \n    def __init__(self):\n        if not PLOTLY_AVAILABLE:\n            raise ImportError(\"Plotly is required for interactive plotting\")\n    \n    def plot_interactive_forecast(self, \n                                 actual: List[float],\n                                 predicted: List[float],\n                                 train_data: Optional[List[float]] = None,\n                                 confidence_intervals: Optional[Dict[str, List[float]]] = None,\n                                 title: str = \"Interactive Forecast\") -> go.Figure:\n        \"\"\"\n        Create interactive forecast plot\n        \n        Args:\n            actual: Actual values\n            predicted: Predicted values\n            train_data: Training data (optional)\n            confidence_intervals: Confidence intervals (optional)\n            title: Plot title\n            \n        Returns:\n            Plotly figure\n        \"\"\"\n        fig = go.Figure()\n        \n        # Add training data if provided\n        if train_data:\n            train_x = list(range(len(train_data)))\n            fig.add_trace(go.Scatter(\n                x=train_x,\n                y=train_data,\n                mode='lines',\n                name='Training Data',\n                line=dict(color='blue', width=2),\n                opacity=0.7\n            ))\n            forecast_start = len(train_data)\n        else:\n            forecast_start = 0\n        \n        # Add forecast data\n        forecast_x = list(range(forecast_start, forecast_start + len(actual)))\n        \n        # Add confidence intervals if provided\n        if confidence_intervals:\n            fig.add_trace(go.Scatter(\n                x=forecast_x + forecast_x[::-1],\n                y=confidence_intervals['upper'] + confidence_intervals['lower'][::-1],\n                fill='toself',\n                fillcolor='rgba(255,0,0,0.2)',\n                line=dict(color='rgba(255,255,255,0)'),\n                name='Confidence Interval',\n                showlegend=True\n            ))\n        \n        # Add actual values\n        fig.add_trace(go.Scatter(\n            x=forecast_x,\n            y=actual,\n            mode='lines+markers',\n            name='Actual',\n            line=dict(color='green', width=3),\n            marker=dict(size=6)\n        ))\n        \n        # Add predicted values\n        fig.add_trace(go.Scatter(\n            x=forecast_x,\n            y=predicted,\n            mode='lines+markers',\n            name='Predicted',\n            line=dict(color='red', width=3, dash='dash'),\n            marker=dict(size=6, symbol='square')\n        ))\n        \n        # Add vertical line for forecast start\n        if train_data:\n            fig.add_vline(\n                x=forecast_start,\n                line_dash=\"dot\",\n                line_color=\"black\",\n                annotation_text=\"Forecast Start\"\n            )\n        \n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Time\",\n            yaxis_title=\"Value\",\n            hovermode='x unified',\n            template='plotly_white'\n        )\n        \n        return fig\n    \n    def plot_interactive_comparison(self, \n                                   results: Dict[str, Dict[str, Any]],\n                                   metric: str = 'mase',\n                                   title: str = \"Interactive Model Comparison\") -> go.Figure:\n        \"\"\"\n        Create interactive model comparison plot\n        \n        Args:\n            results: Model results dictionary\n            metric: Metric to compare\n            title: Plot title\n            \n        Returns:\n            Plotly figure\n        \"\"\"\n        model_names = []\n        values = []\n        \n        for model_name, model_results in results.items():\n            if metric in model_results:\n                value = model_results[metric]\n                if not np.isnan(value) and not np.isinf(value):\n                    model_names.append(model_name)\n                    values.append(value)\n        \n        # Sort by performance\n        sorted_data = sorted(zip(model_names, values), key=lambda x: x[1])\n        model_names, values = zip(*sorted_data)\n        \n        # Create color scale (green for best, red for worst)\n        colors = px.colors.sample_colorscale('RdYlGn_r', len(values))\n        \n        fig = go.Figure(data=[\n            go.Bar(\n                x=list(model_names),\n                y=list(values),\n                marker_color=colors,\n                text=[f'{v:.4f}' for v in values],\n                textposition='auto',\n            )\n        ])\n        \n        fig.update_layout(\n            title=title,\n            xaxis_title=\"Model\",\n            yaxis_title=metric.upper(),\n            template='plotly_white'\n        )\n        \n        return fig\n\n\ndef create_forecast_report(actual: List[float],\n                          predicted: List[float],\n                          train_data: Optional[List[float]] = None,\n                          model_name: str = \"Chronos\",\n                          save_dir: str = \"forecast_report\") -> str:\n    \"\"\"\n    Create a comprehensive forecast report with multiple visualizations\n    \n    Args:\n        actual: Actual values\n        predicted: Predicted values\n        train_data: Training data (optional)\n        model_name: Name of the forecasting model\n        save_dir: Directory to save report files\n        \n    Returns:\n        Path to the main report file\n    \"\"\"\n    import os\n    \n    # Create directory\n    os.makedirs(save_dir, exist_ok=True)\n    \n    visualizer = ForecastVisualizer()\n    \n    # 1. Main forecast plot\n    fig1 = visualizer.plot_forecast_results(\n        actual, predicted, train_data,\n        title=f\"{model_name} Forecast Results\",\n        save_path=os.path.join(save_dir, \"forecast_results.png\")\n    )\n    plt.close(fig1)\n    \n    # 2. Residual analysis\n    fig2 = visualizer.plot_residual_analysis(\n        actual, predicted,\n        title=f\"{model_name} Residual Analysis\",\n        save_path=os.path.join(save_dir, \"residual_analysis.png\")\n    )\n    plt.close(fig2)\n    \n    # 3. Calculate metrics\n    from sklearn.metrics import mean_squared_error, mean_absolute_error\n    \n    mse = mean_squared_error(actual, predicted)\n    mae = mean_absolute_error(actual, predicted)\n    rmse = np.sqrt(mse)\n    mape = np.mean(np.abs((np.array(actual) - np.array(predicted)) / (np.array(actual) + 1e-8))) * 100\n    \n    # 4. Create HTML report\n    html_content = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>{model_name} Forecast Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n            h1 {{ color: #333; }}\n            h2 {{ color: #666; }}\n            .metric {{ background-color: #f0f0f0; padding: 10px; margin: 10px 0; border-radius: 5px; }}\n            .metric-value {{ font-weight: bold; color: #2c3e50; }}\n            img {{ max-width: 100%; height: auto; margin: 20px 0; }}\n        </style>\n    </head>\n    <body>\n        <h1>{model_name} Forecast Report</h1>\n        \n        <h2>Performance Metrics</h2>\n        <div class=\"metric\">Mean Squared Error (MSE): <span class=\"metric-value\">{mse:.6f}</span></div>\n        <div class=\"metric\">Root Mean Squared Error (RMSE): <span class=\"metric-value\">{rmse:.6f}</span></div>\n        <div class=\"metric\">Mean Absolute Error (MAE): <span class=\"metric-value\">{mae:.6f}</span></div>\n        <div class=\"metric\">Mean Absolute Percentage Error (MAPE): <span class=\"metric-value\">{mape:.2f}%</span></div>\n        \n        <h2>Forecast Results</h2>\n        <img src=\"forecast_results.png\" alt=\"Forecast Results\">\n        \n        <h2>Residual Analysis</h2>\n        <img src=\"residual_analysis.png\" alt=\"Residual Analysis\">\n        \n        <h2>Summary</h2>\n        <p>This report shows the forecasting performance of the {model_name} model.</p>\n        <p>Total predictions: {len(predicted)}</p>\n        <p>Training data points: {len(train_data) if train_data else 'N/A'}</p>\n        \n    </body>\n    </html>\n    \"\"\"\n    \n    report_path = os.path.join(save_dir, \"forecast_report.html\")\n    with open(report_path, 'w') as f:\n        f.write(html_content)\n    \n    print(f\"Forecast report saved to: {report_path}\")\n    return report_path\n\n\ndef test_visualization():\n    \"\"\"\n    Test visualization functions\n    \"\"\"\n    print(\"Testing Visualization Module...\")\n    \n    # Generate synthetic data\n    np.random.seed(42)\n    n_train = 50\n    n_test = 10\n    \n    # Training data\n    train_data = np.cumsum(np.random.randn(n_train)) + 100\n    \n    # Test data\n    actual = np.cumsum(np.random.randn(n_test)) + train_data[-1]\n    predicted = actual + np.random.normal(0, 0.5, n_test)\n    \n    print(f\"Generated data: {n_train} train, {n_test} test points\")\n    \n    # Test visualizer\n    visualizer = ForecastVisualizer()\n    \n    # 1. Test time series plot\n    print(\"\\n1. Testing time series plot...\")\n    fig1 = visualizer.plot_time_series(\n        train_data.tolist(),\n        title=\"Test Time Series\",\n        show_trend=True\n    )\n    plt.show()\n    \n    # 2. Test forecast results plot\n    print(\"\\n2. Testing forecast results plot...\")\n    fig2 = visualizer.plot_forecast_results(\n        actual.tolist(),\n        predicted.tolist(),\n        train_data.tolist(),\n        title=\"Test Forecast Results\"\n    )\n    plt.show()\n    \n    # 3. Test residual analysis\n    print(\"\\n3. Testing residual analysis...\")\n    fig3 = visualizer.plot_residual_analysis(\n        actual.tolist(),\n        predicted.tolist(),\n        title=\"Test Residual Analysis\"\n    )\n    plt.show()\n    \n    # 4. Test model comparison\n    print(\"\\n4. Testing model comparison...\")\n    mock_results = {\n        'Model A': {'mse': 0.5, 'mae': 0.3, 'mape': 5.2},\n        'Model B': {'mse': 0.7, 'mae': 0.4, 'mape': 6.1},\n        'Model C': {'mse': 0.3, 'mae': 0.2, 'mape': 4.8}\n    }\n    \n    fig4 = visualizer.plot_model_comparison(\n        mock_results,\n        title=\"Test Model Comparison\"\n    )\n    plt.show()\n    \n    # 5. Test feature importance\n    print(\"\\n5. Testing feature importance...\")\n    mock_importance = {\n        'feature_1': 0.25,\n        'feature_2': 0.18,\n        'feature_3': 0.15,\n        'feature_4': 0.12,\n        'feature_5': 0.10,\n        'feature_6': 0.08,\n        'feature_7': 0.07,\n        'feature_8': 0.05\n    }\n    \n    fig5 = visualizer.plot_feature_importance(\n        mock_importance,\n        title=\"Test Feature Importance\"\n    )\n    plt.show()\n    \n    # 6. Test interactive plotting (if available)\n    if PLOTLY_AVAILABLE:\n        print(\"\\n6. Testing interactive plotting...\")\n        interactive_plotter = InteractivePlotter()\n        \n        fig_interactive = interactive_plotter.plot_interactive_forecast(\n            actual.tolist(),\n            predicted.tolist(),\n            train_data.tolist(),\n            title=\"Test Interactive Forecast\"\n        )\n        \n        # Save as HTML\n        fig_interactive.write_html(\"test_interactive_forecast.html\")\n        print(\"   Interactive plot saved as 'test_interactive_forecast.html'\")\n    \n    # 7. Test report generation\n    print(\"\\n7. Testing report generation...\")\n    report_path = create_forecast_report(\n        actual.tolist(),\n        predicted.tolist(),\n        train_data.tolist(),\n        model_name=\"Test Model\",\n        save_dir=\"test_report\"\n    )\n    \n    print(\"\\nVisualization module test completed!\")\n    return visualizer\n\n\nif __name__ == \"__main__\":\n    # Run tests\n    visualizer = test_visualization()