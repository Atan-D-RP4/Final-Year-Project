// Summary section
== Summary
We propose to explore the application of *Chronos-Bolt*, a pretrained foundation model for probabilistic time-series forecasting, to a novel sequential domain such as *user clickstream behavior* or *dialogue sentiment streams*. By discretizing real-valued sliding-window features (e.g., event counts, sentiment scores) into token sequences, we treat these behavioral signals as a pseudo-time-series “language,” in line with Chronos’s quantization-based tokenization approach
                                                                                                                                     Our primary goal is to evaluate Chronos-Bolt’s *zero-shot forecasting performance* on the chosen domain—i.e., applying the pretrained model without fine-tuning. If time and computational resources permit, we will optionally fine-tune Chronos-Bolt on tokenized sequences to assess gains in accuracy using AutoGluon-TimeSeries infrastructure

// Research questions
== Research Questions
- Can zero-shot Chronos-Bolt predict future behavioral tokens (e.g., next event type, sentiment shift) with acceptable accuracy?     - Does fine-tuning yield measurable improvements in forecasting performance for domain-specific sequential data?

// Methodology
== Methodology                                                                                                                       1. *Domain & dataset selection*: Choose one domain (e.g., Kaggle clickstream, conversation logs with sentiment scores).              2. *Data preprocessing*: Compute sliding-window metrics, normalize scales, discretize into fixed token vocabulary.                   3. *Zero-shot evaluation*: Use Chronos-Bolt via AutoGluon to forecast target token sequences; compute metrics like classification accuracy (F1, AUC) or quantile forecast accuracy (MASE, CRPS) depending on output type.                                                 4. *Optional fine-tuning*: If feasible, fine-tune Chronos-Bolt on a training split using AutoGluon’s `fine_tune=True` setting and compare to zero-shot baseline                                                                                                           
// Expected contributions
== Expected Contributions
- Demonstrate whether Chronos’s pre-trained capabilities transfer to behavioral or textual-like sequential domains without retraining.                                                                                                                                    - Show feasibility of token-based forecasting in non-traditional time series.
- Provide comparisons of zero-shot vs fine-tuned performance in a real sequential domain. (Optionally, based on time available)                                                                                                                                           // Scope and constraints
== Scope & Constraints
- Focus on a single domain to manage complexity within a 5-month undergraduate team project.
- Zero-shot evaluation is mandatory; fine-tuning is optional and contingent on resource availability and schedule.                   - Use pre-trained Chronos-Bolt models (e.g., mini, small, or base) to avoid heavy training overhead and simplify pipeline integration via AutoGluon.                                                                                                                                                                                                                                                           // References
= References                                                                                                                         #set enum(numbering: "[1]")                                                                                                          + Amazon Web Services, Inc., “Fast and accurate zero-shot forecasting with Chronos-Bolt and AutoGluon | Artificial Intelligence,” #link("https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/") #label("aws-blog")                                                                                                                           + Amazon Science, “Chronos: Adapting language model architectures for time series forecasting,” #link("https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting/") #label("amazon-science")                                      + HKU SPACE AI Hub, “Fast and accurate zero-shot forecasting with Chronos-Bolt and AutoGluon,” #link("https://aihub.hkuspace.hku.hk/2024/12/03/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/") #label("hku-ai-hub")
+ “Chronos: Learning the Language of Time Series,” #link("https://arxiv.org/abs/2403.07815") #label("arxiv")
+ Reddit, “[R] Chronos: Learning the Language of Time Series,” #link("https://www.reddit.com/r/MachineLearning/comments/1behp7t") #label("reddit")
+ Hugging Face, “amazon/chronos-bolt-small,” #link("https://huggingface.co/amazon/chronos-bolt-small") #label("huggingface")